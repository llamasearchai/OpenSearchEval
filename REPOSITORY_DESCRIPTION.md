# OpenSearchEval Repository

## Description

A comprehensive, production-ready platform for evaluating search quality, conducting A/B tests, and analyzing user behavior. Built with modern Python technologies and featuring agent architecture, FastAPI endpoints, and MLX integration for Apple Silicon optimization.

## Repository Summary

OpenSearchEval is a powerful search evaluation framework designed for modern search systems. It provides comprehensive metrics, A/B testing capabilities, and advanced analytics to help you understand and improve your search performance.

### Core Capabilities

- Search Quality Metrics (MRR, NDCG, Precision@K, Recall@K)
- A/B Testing Framework with statistical significance
- User Behavior Analytics and journey analysis
- Agent Architecture with distributed processing
- MLX Integration for Apple Silicon optimization
- LLM-as-Judge for AI-powered evaluation
- FastAPI REST API with automatic documentation
- Rich visualizations and reporting tools
- Extensible plugin architecture
- Real-time performance monitoring

### Technology Stack

- **Backend**: Python 3.9+, FastAPI, SQLAlchemy
- **ML/AI**: MLX, OpenAI API, Transformers
- **Database**: PostgreSQL, Redis
- **Frontend**: Flask, JavaScript, HTML5/CSS3
- **Deployment**: Docker, Docker Compose
- **Testing**: pytest, coverage.py
- **Documentation**: Sphinx, ReadTheDocs

### Author

Nik Jois <nikjois@llamasearch.ai>
LlamaSearch AI

### License

MIT License - see LICENSE file for details 